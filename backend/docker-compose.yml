version: '3.9'

services:
  postgres:
    image: postgres:14-alpine
    container_name: pms-postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./migrations:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - pms-network
    restart: unless-stopped

  redis:
    env_file:
      - ./.env
    image: redis:7-alpine
    container_name: pms-redis
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    networks:
      - pms-network
    restart: unless-stopped

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: pms-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_MAX_CLIENT_CNXNS: 0
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - pms-network
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: pms-kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 4
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_MS: 604800000
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - pms-network
    restart: unless-stopped

  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: pms-kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic product.created --partitions 4 --replication-factor 1 --config retention.ms=604800000
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic product.updated --partitions 4 --replication-factor 1 --config retention.ms=604800000
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic product.deleted --partitions 4 --replication-factor 1 --config retention.ms=604800000
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic product.lowstock --partitions 4 --replication-factor 1 --config retention.ms=604800000
      kafka-topics --bootstrap-server kafka:29092 --list
      "
    networks:
      - pms-network

  localstack:
    env_file:
      - ./.env
    image: localstack/localstack:latest
    container_name: pms-localstack
    ports:
      - "4566:4566"
      - "4510-4559:4510-4559"
    environment:
      SERVICES: s3,dynamodb
      DEBUG: 0
      DATA_DIR: /var/lib/localstack
      TMPDIR: /var/lib/localstack/tmp
      AWS_DEFAULT_REGION: ${AWS_REGION}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    volumes:
      - localstack_data:/var/lib/localstack
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 20s
    networks:
      - pms-network
    restart: unless-stopped

  localstack-init:
    env_file:
      - ./.env
    image: amazon/aws-cli:latest
    container_name: pms-localstack-init
    depends_on:
      localstack:
        condition: service_healthy
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_REGION}
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      aws --endpoint-url=http://localstack:4566 s3 mb s3://product-analytics-archive || true
      aws --endpoint-url=http://localstack:4566 dynamodb create-table \
        --table-name ProductNotifications \
        --attribute-definitions AttributeName=notificationId,AttributeType=S AttributeName=timestamp,AttributeType=N \
        --key-schema AttributeName=notificationId,KeyType=HASH AttributeName=timestamp,KeyType=RANGE \
        --billing-mode PAY_PER_REQUEST \
        --stream-specification StreamEnabled=false || true
      aws --endpoint-url=http://localstack:4566 dynamodb create-table \
        --table-name ProductAnalytics \
        --attribute-definitions AttributeName=sellerId,AttributeType=S AttributeName=dateKey,AttributeType=S \
        --key-schema AttributeName=sellerId,KeyType=HASH AttributeName=dateKey,KeyType=RANGE \
        --billing-mode PAY_PER_REQUEST \
        --stream-specification StreamEnabled=false || true
      aws --endpoint-url=http://localstack:4566 s3 ls
      aws --endpoint-url=http://localstack:4566 dynamodb list-tables
      "
    networks:
      - pms-network

  api-server:
    env_file:
      - ./.env
    build:
      context: .
      dockerfile: Dockerfile
      target: api-server
    container_name: pms-api-server
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
      localstack:
        condition: service_healthy
    ports:
      - "3001:3001"
    environment:
      NODE_ENV: ${NODE_ENV}
      PORT: 3001
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      KAFKA_BROKERS: kafka:29092
      KAFKA_CLIENT_ID: api-server
      AWS_ENDPOINT: http://localstack:4566
      AWS_REGION: ${AWS_REGION}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      RATE_LIMIT_STANDARD: ${RATE_LIMIT_STANDARD}
      RATE_LIMIT_BULK: ${RATE_LIMIT_BULK}
      LOW_STOCK_THRESHOLD: ${LOW_STOCK_THRESHOLD}
    volumes:
      - .:/app
      - /app/node_modules
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3001/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - pms-network
    restart: unless-stopped

  notification-service:
    env_file:
      - ./.env
    build:
      context: .
      dockerfile: Dockerfile
      target: notification-service
    container_name: pms-notification-service
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
      localstack:
        condition: service_healthy
    environment:
      NODE_ENV: ${NODE_ENV}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      KAFKA_BROKERS: kafka:29092
      KAFKA_CLIENT_ID: notification-service
      KAFKA_GROUP_ID: notification-service-group
      AWS_ENDPOINT: http://localstack:4566
      AWS_REGION: ${AWS_REGION}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      DYNAMODB_NOTIFICATIONS_TABLE: ProductNotifications
      MAX_RETRIES: 5
      INITIAL_RETRY_DELAY_MS: 500
      BACKOFF_MULTIPLIER: 2
      MAX_RETRY_DELAY_MS: 10000
    volumes:
      - .:/app
      - /app/node_modules
    healthcheck:
      test: ["CMD", "pgrep", "-f", "node"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - pms-network
    restart: unless-stopped

  analytics-service:
    env_file:
      - ./.env
    build:
      context: .
      dockerfile: Dockerfile
      target: analytics-service
    container_name: pms-analytics-service
    depends_on:
      kafka:
        condition: service_healthy
      localstack:
        condition: service_healthy
    environment:
      NODE_ENV: ${NODE_ENV}
      KAFKA_BROKERS: kafka:29092
      KAFKA_CLIENT_ID: analytics-service
      KAFKA_GROUP_ID: analytics-service-group
      AWS_ENDPOINT: http://localstack:4566
      AWS_REGION: ${AWS_REGION}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      DYNAMODB_ANALYTICS_TABLE: ProductAnalytics
      S3_ANALYTICS_BUCKET: product-analytics-archive
      HOT_STORAGE_DAYS: 30
      WORKER_THREADS_COUNT: 4
      MAX_RETRIES: 10
      INITIAL_RETRY_DELAY_MS: 2000
      BACKOFF_MULTIPLIER: 2
      MAX_RETRY_DELAY_MS: 60000
    volumes:
      - .:/app
      - /app/node_modules
    healthcheck:
      test: ["CMD", "pgrep", "-f", "node"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - pms-network
    restart: unless-stopped

networks:
  pms-network:
    driver: bridge
    name: pms-network

volumes:
  postgres_data:
    name: pms-postgres-data
  redis_data:
    name: pms-redis-data
  zookeeper_data:
    name: pms-zookeeper-data
  zookeeper_log:
    name: pms-zookeeper-log
  kafka_data:
    name: pms-kafka-data
  localstack_data:
    name: pms-localstack-data
